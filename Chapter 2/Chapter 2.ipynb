{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the backpropagation algorithm works\n",
    "\n",
    "In Chapter 1 we saw how neural networks can learn their weights and biases using the gradient descent algorithm, but we didn't discuss how to compute the gradient of the cost function!\n",
    "\n",
    "The backpropagation algorithm was originally introduced in the 70s, but not until a [famous paper](https://www.academia.edu/2520405/Learning_representations_by_back-propagating_errors?auto=download) by Rumelhart, Hinton and Williams described several neural networks where backpropagation works way faster than earlier approaches to learning, making it possible to use neural nets to solve previously insoluble problems.\n",
    "\n",
    "This chapter is more mathematically involved than the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of backpropagation is an expression for the partial derivative of the cost function C with respect to any weight w or bias b in the network. And while the expression is complex, it also has a beauty to it, with each element having a natural, intuitive interpretation.\n",
    "\n",
    "Backpropagation isn't just a fast algorithm for learning. It actually gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up\n",
    "\n",
    "Let's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use $w^{l}_{jk}$ to denote the weight for the connection from the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "\n",
    "We use a similar notation for the network's biases and activations. Let's use $b^{l}_{j}$ for the bias in the $j^{th}$ neuron in the $l^{th}$ layer. \n",
    "\n",
    "And we use $a^{l}_{j}$ for the activation in the $j^{th}$ neuron in the $l^{th}$ layer. Remember that the activation is the result of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these notations, the activation $a^{l}_{j}$ of the $j^{th}$ neuron in the $l^{th}$ layer is related to the activations in the $(l-1)^{th}$ layer by:\n",
    "\n",
    "$$a^{l}_{j}=\\sigma(\\sum_{k}w^{l}_{jk}a^{l-1}_{j}+b^{l}_{j})$$\n",
    "\n",
    "where the sum is over all neurons `k` in the $(l-1)^{th}$ layer.\n",
    "\n",
    "To rewrite this expression in a matrix form we define a *weight matrix* $w^{l}$ for each layer `l`, where the entries are just the weights connecting the $l^{th}$ layer of neurons: the entry in the $j^{th}$ row and the $k^{th}$ column is $w^{l}_{jk}$.\n",
    "\n",
    "Similarly, for each layer `l` we define a *bias vector* $b^{l}$, where the components are just the values $b^{l}_{j}$, one component for each neuron in `l`.\n",
    "\n",
    "We also define an *activation vector* $a^{l}$, where the components are just the values $a^{l}_{j}$, one component for each neuron in `l`.\n",
    "\n",
    "One last thing is to treat $\\sigma$ as a vector in the sense of applying the function to each component of the vector we are applying the function to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these notations in mind, the previous equation can be rewritten as\n",
    "\n",
    "$$a^{l} = \\sigma(w^{l}a^{l-1} + b^{l}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expression gives us a much more global way of thinking about how the activations in one layer relate to activations in the previous layer: we just apply the weight matrix to the activations, then add the bias vector, and finally apply the $\\sigma$ function.\n",
    "\n",
    "The global view is often easier and more succint than the neuron-by-neuron view we've taken to now. Think of it as a way of escaping index hell, while remaining precise about what is going on. The expression is also useful, because most matrix libraries provide fast ways of implementing matrix multiplication, vector addition, and vectorization.\n",
    "\n",
    "When using this equation to compute $a^{l}$, we compute the intermediate quantity $z^{l}\\equiv w^{l}a^{l-1} + b^{l}$ along the way, what we call the *weighted input* to the neurons in layer `l`, with components $z^{l}_{j}=\\sum_{k}w^{l}_{jk}a^{l-1}_{j}+b^{l}_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of backpropagation is to compute the partial derivatives $\\frac{\\partial C}{\\partial w}$ and $\\frac{\\partial C}{\\partial b}$ of the cost function C with respect to any weight `w` or bias `b` in the network.\n",
    "\n",
    "For backpropagation to work we need to make two main assumptions about the form of the cost function. Let's have the quadratic cost function in mind as an example:\n",
    "\n",
    "$$ C = \\frac{1}{2n}\\sum_{x} || y(x) - a^{L}(x) ||^{2} $$\n",
    "\n",
    "where `n` is the total number of training examples, the sum is over each training example `x`, `y(x)` is the corresponding desired output, `L` the number of layers in the network, and $a^{L}(x)$ is the vector of activations output from the network when x is input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C can be written as an average over cost functions for individual training examples\n",
    "\n",
    "$$ C = \\frac{1}{n}\\sum_{x} C_{x} $$\n",
    "\n",
    "The reason we need this assumption is because what backpropagation actually let us do is compute the partial derivatives $\\frac{\\partial C_{x}}{\\partial w}$ and $\\frac{\\partial C_{x}}{\\partial b}$ for a single training example, and then we recover $\\frac{\\partial C}{\\partial w}$ and $\\frac{\\partial C}{\\partial b}$ by averaging over all training examples.\n",
    "\n",
    "### C can be written as a function of the outputs from the neural network\n",
    "\n",
    "$$ C = C(a^{L}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
