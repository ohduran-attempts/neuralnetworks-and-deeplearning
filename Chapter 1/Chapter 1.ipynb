{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centerpiece is a `Network` class, which we use to represent a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x, y  in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the list `sizes` contains the number of neurons in the respective layers. So, for example, if we want to create a `Network` object with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer, we'd do this with:\n",
    "\n",
    "    net = Network([2,3,1])\n",
    "\n",
    "The biases and weights in the `Network` object are all initialised randomly, generating Gaussian distirbutions with mean 0 and standard deviation 1 (stochastic gradient descent has now a place to start from - not optimal, but will do for now).\n",
    "\n",
    "We assume that the biases and weights are stored as lists of numpy matrices, and that the first layer of neurons is the input layer (omitting any biases for those neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 +np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the input `z` is a vector, numpy automatically applies the function `sigmoid` element wise (in vectorised form).\n",
    "\n",
    "We then add a `feedforward` method to the `Network` class, which, given an input `a` for the network, returns the correspondent output (assumed to be an (n,1) numpy array, where n is the number of inputs in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    \"\"\"Return the output of the network if a is input.\"\"\"\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a) + b)\n",
    "\n",
    "Network.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a' = sigmoid(W * a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to create a new method that implements the stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \"\"\"\n",
    "    Train the neural network using mini-batch stochastic gradient descent.\n",
    "    \n",
    "    The training_data is a list of tuples (x,y) representing the training inputs\n",
    "    and the desired outputs.\n",
    "    If test data is provided then the network will be evaluated against the test data\n",
    "    after each epoch, and partial progress will be printed out.\n",
    "\n",
    "    \"\"\"\n",
    "    if test_data:\n",
    "        n_test = len(test_data)\n",
    "    \n",
    "    n = len(training_data)\n",
    "    for j in range(epochs):\n",
    "        ramdom.shuffle(training_data)\n",
    "        mini_batches - [\n",
    "            training_dta[k:k+mini_batch_size]\n",
    "            for k in range(0, n, mini_batch_size)\n",
    "        ]\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        if test_data:\n",
    "            print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "Network.SGD = SGD\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works as follows: in each epoch, it starts by randomly shuffling the training data, and then partitions it into mini batches of the appropriate size, which is an easy way to sampling randomly from the training data. Then for each `mini_batch` we apply a single step of gradient descent.\n",
    "\n",
    "This is done by means of `self.update_mini_batch`, which updates the network weights and biases according to a single iteration of gradient descent, using just the training data in `mini_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mini_batch(self, mini_batch, eta):\n",
    "    \"\"\"\n",
    "    Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation to a single mini_batch.\n",
    "    eta is the learning_rate.\n",
    "    \"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "        nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "    self.weights = [w - (eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b - (eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "Network.update_mini_batch = update_mini_batch        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `backprop` is doing pretty much all the heavy lifting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "    \"\"\"\n",
    "    Return a tuple (nabla_b, nabla_w) representing the gradient for the cost function C_x.\n",
    "    \n",
    "    nabla_b and nabla_w are layer-by -layer lists of numpy arrays, similar to self.biases and self.weights.\n",
    "    \"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    #Feedforward\n",
    "    activation = x\n",
    "    activations = [x] # activations will be added here layer by layer\n",
    "    \n",
    "    zs = []\n",
    "    \n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    # Backward Pass\n",
    "    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    \n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    # Note that the variable l below is used backwards: l1 is the last layer, l2 is the second to last, etc.\n",
    "    for l in range(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weigts[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "Network.backprop = backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method makes use of a few extra functions to help in computing the gradient, namely `sigmoid_prime`, which computes the derivative of the sigmoid function, and `cost_derivative`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "Network.sigmoid_prime = sigmoid_prime\n",
    "\n",
    "def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "Network.cost_derivative = cost_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-69c65344baec>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Alvaro.DuranBarata\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Alvaro.DuranBarata\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Alvaro.DuranBarata\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Alvaro.DuranBarata\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Alvaro.DuranBarata\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = mnist.train\n",
    "mnist.train.next_batch(100)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson to take away from this is that debugging a neural network is not trivial, and, just as for ordinary programming, there is an art to it. You need to learn that art of debugging in order to get good results from neural networks. More generally, we need to develop heuristics for choosing good hyper-parameters and a good architecture. We'll discuss all these at length through the book, including how I chose the hyper-parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
